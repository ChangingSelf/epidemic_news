# epidemic_news

爬取[学校网站新闻](http://www.chd.edu.cn/yqfk/)

[幕布链接](https://mubu.com/doc/2MY36Dq0r2u)：项目遇到的一些问题以及解决方案



## 需要爬取的字段

- [ ] 栏目ID：未知
- [x] 文章标题
- [x] 文章url
- [ ] 缩略图：每篇文章中第一张图的url
- [ ] 标签：未知
- [ ] 创建时间：不知道怎么获取
- [ ] 更新时间：不知道怎么获取
- [ ] 发布时间
- [ ] 权限：填写为full



## 使用

命令行移动到项目文件夹中：

```bash
$scrapy crawl schoolNews -o test.json
```

用了这个之后，就会爬取【学校新闻】版块的内容到test.json里面，目前爬取到的字段在上面勾选了

## Spider部分

### 部分名称详细

为了防止部分名称引发歧义导致不必要的误解,下面做些详细的描述

- **主网站:** http://www.chd.edu.cn/yqfk/main.htm
- **(一级)子网站:** 主网站下的六个子网站(分别对应六个板块),也就是爬取文章列表的地方
- **二级子网站:** 也就是具体文章所在的页面(网站)

### 图片

- 图片需要单独进行爬取,定义一个函数`request_imgs(self,response,imgs)`用于构造图片的请求

- 定义一个图片解析函数,`parse_img(self, response)`,进行图片下载/保存

## 写的过程中做的一些记录(hfldqwe)

- 解析函数里面还是有很多代码都是重复了(比如标题,索引,版块名称这些地方),可以进行一波修改(等写完了再改吧)

- 之前找里面域名可能出了点问题,其实没有那么多. 但是有新增的页面(但之后持续测试过程中又出现了不一样的地方,本来没有找到的域名,又重新出现了,也不知道是恰巧碰到了网站更新还是咋的,这里先记录着)

- **很奇怪:有时候会出现对应的文章,有时候没有.**


## 流程

### Spider爬取部分
1. 分别爬取六个一级子版块网站

2. 提取对应的文章标题,日期,以及链接

3. 通过redis判断链接是否已经爬取, 是则停止提取, 并重复1~3步, 否则第4步

4. 将文章链接构造为Request, 并通过域名判断使用不同的文章解析函数进行解析提取(图片类似)

### 字段部分
1. 使用Itemloader进行处理

### 管道部分
1. pass

### 数据库部分(涉及三个数据表)
- **fa_cms_archives**: 主要用到的表, 版块id,标题,时间等等字段
- **fa_cms_addonnews**: 写入内容的表,id与`fa_cms_archives`对应
- **fa_cms_tags**: 写入标签的表

1. 判断`archives`中是否有重复数据(根据标题,标签,时间判断), 如果重复, 判断power是否一致, 一致则写入一个Error级别日志, 
不一致则按照更低级别(如all)进行更新

2. 没有重复数据,则将数据写入到`archives`表中

3. 获取此数据在`archives`表中的id

4. 将对应数据插入`addonnews`表中

5. 判断`tags`表中是否存在此标签, 不存在则创建, 存在则更新








## 结构设计

### SchoolnewsSpider.py说明

- `start_url`列表里面存放六个一级子网站的url，依次解析这几个子网站

- `SchoolnewsSpider.parser()`解析一级子网站（目录）下的所有文章url，并根据域名生成不同的Request，获取完本页会继续获取下一页
  - 域名映射字典`self.parser_domain_map`里面存放着每个域名对应的解析回调函数，在生成Request时传入
  - 默认解析函数`self.default_parser`是在域名映射字典未找到对应的域名时，使用的解析函数，可以用于测试单个文章解析函数

- 去重